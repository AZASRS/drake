---
title: "High-performance computing with drake"
author: "Will Landau"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    toc: true
    number_sections: true
vignette: >
  %\VignetteIndexEntry{parallelism}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r suppression, echo = F}
suppressMessages(suppressWarnings(library(future)))
suppressMessages(suppressWarnings(library(drake)))
suppressMessages(suppressWarnings(library(dplyr)))
clean(destroy = TRUE, verbose = FALSE)
unlink(c("Makefile", "report.Rmd", "shell.sh", "STDIN.o*", "Thumbs.db"))
knitr::opts_chunk$set(
  collapse = TRUE,
  error = TRUE,
  warning = TRUE
)
```

`Drake` is not only a reproducibility tool, but also a high-performance computing engine. To activate parallel computing, just set the `jobs` argument of `make()` to a value greater than 1. Below, up to 2 targets can run simultaneously at any given time.

```{r a, eval = FALSE}
library(drake)
load_mtcars_example()
make(my_plan, jobs = 2)
```

To deploy serious long workflows, we recommend putting the call to `make()` in a script (say, `make.R`) and running it in an unobtrusive background process that persists after you log out. In the Linux command line, this is straightforward.

<pre><code>nohup nice -19 R CMD BATCH make.R &
</code></pre>

When you deploy your project, `drake` uses the dependency network to figure out how to run your work in parallel. You as the user do not have to micromanage when individual targets are built.

<iframe
src = "https://ropensci.github.io/drake/images/outdated.html"
width = "100%" height = "600px" allowtransparency="true"
style="border: none; box-shadow: none">
</iframe>

There are multiple ways to walk this graph and multiple ways to launch workers, and every project has its own needs. Thus, `drake` supports multiple parallel backends. Choose the backend with the `parallelism` argument.

```{r b, eval = FALSE}
make(my_plan, parallelism = "parLapply", jobs = 2)
```

List your options with `parallelism_choices()`.

```{r choices}
parallelism_choices()
```

The backends vary widely in terms of how the workers deploy and how they are scheduled.

|                       | Deploy: local | Deploy: remote |
| --------------------- |:-------------:| -----:|
| Schedule: persistent | "mclapply", "parLapply" | "future_lapply" |
| Schedule: transient  | | "future", "Makefile" |
| Schedule: staged     | "mclapply_staged", "parLapply_staged" | |

The next sections describe how and when to use each scheduling algorithm and deployment strategy.

# Local workers

Local workers deploy as separate forks or processes to you computer. The `"mclapply"` and `"mclapply_staged"` backends uses the `mclapply()` function from the `parallel` package to launch workers. 

```{r local1, eval = FALSE}
make(my_plan, parallelism = "mclapply", jobs = 2)
make(my_plan, parallelism = "mclapply_staged", jobs = 2)
```

Workers are quicker to launch than in any other `drake` backend, so these two choices are the lowest-overhead options. However, they have limitations: the `mclapply()` function is inefficient with respect to computer memory (see explanations [here](https://github.com/tdhock/mclapply-memory) and [here](http://lcolladotor.github.io/2013/11/14/Reducing-memory-overhead-when-using-mclapply/#.Wu8Fc9Yh1hE)) and it cannot launch multiple workers on Windows. For this reason, `drake` supports platform agnostic backends `"parLapply"` and `"parLapply_staged"`, both of which are based on the `parLapply()` function from the `parallel` package. These options work on Windows, but each `make()` requires extra overhead to create a [parallel socket (PSOCK) cluster](http://gforge.se/2015/02/how-to-go-parallel-in-r-basics-tips/).

```{r local2, eval = FALSE}
make(my_plan, parallelism = "parLapply", jobs = 2)
make(my_plan, parallelism = "parLapply_staged", jobs = 2)
```

The default parallelism is `"parLapply"` on Windows and `"mclapply"` everywhere else.

```{r defaultparallelism}
default_parallelism()
```

# Remote workers

The `"future_lapply"`, `"future"`, and `"Makefile"` backends have the option to launch workers to remote resources such as nodes on a computing cluster.

```{r distributedoptions}
parallelism_choices(distributed_only = TRUE)
```

Testing them out is straightforward.

```{r remote1, eval = FALSE}
make(my_plan, parallelism = "future", jobs = 2)
make(my_plan, parallelism = "future_lapply", jobs = 2)
make(my_plan, parallelism = "Makefile", jobs = 2)
```

For remote workers, the all the imports are processed with one of the local worker backends before any of the targets start. You can use different numbers of workers for the imports and the targets.

```{r remote2, eval = FALSE}
make(my_plan, parallelism = "future", jobs = c(imports = 2, targets = 4))
```

By default, these backends launch the workers on your local machine. It takes extra configuring to actually deploy them to a remote cluster. The next subsections have the details.

## `"future"` and `"future_lapply"`

The `plan()` function from the [`future`](https://github.com/HenrikBengtsson/future) package configures how and where the workers will deploy on the next `make()`. For example, the following code uses [`future`](https://github.com/HenrikBengtsson/future)'s `multisession` backend, which is analogous to `drake`'s `"parLapply"` parallelism.

```{r future1, eval = FALSE}
library(future)
future::plan(multisession)
make(my_plan, parallelism = "future", jobs = 2)
make(my_plan, parallelism = "future_lapply", jobs = 2) # Same tech, different scheduling.
```

To deploy to a cluster (say, a [SLURM](https://slurm.schedmd.com/) cluster), you need the [`batchtools`](https://github.com/mllg/batchtools) and [`future.batchtools`](https://github.com/HenrikBengtsson/future.batchtools) packages.

```{r futurebatchtools, eval = FALSE}
library(future.batchtools)
```

You also need [template file](https://github.com/mllg/batchtools/tree/master/inst/templates) to configure [`batchtools`](https://github.com/mllg/batchtools) the remote resources, such as the memory and wall time limits. Use `drake_batchtools_tmpl_file()` to write one of the examples from the [`drake_example()` files](https://github.com/ropensci/drake/tree/master/inst/examples). You will probably need to edit it manually to match your resources and needs.

```{r exlksjdf, eval = FALSE}
drake_batchtools_tmpl_file("slurm") # Write batchtools.slurm.tmpl.
```

Load the template file your `future::plan()` and call `make()` to run the project.

```{r futurebatchtools2, eval = FALSE}
future::plan(batchtools_slurm, template = "batchtools.slurm.tmpl")
make(my_plan, parallelism = "future", jobs = 2)
make(my_plan, parallelism = "future_lapply", jobs = 2) # Same tech, different scheduling.
```

See packages [`future`](https://github.com/HenrikBengtsson/future), [`future.batchtools`](https://github.com/HenrikBengtsson/future.batchtools), and [`batchtools`](https://github.com/mllg/batchtools) for more options. For example, the alternatives for `future::plan()` are listed [here](https://github.com/HenrikBengtsson/future#controlling-how-futures-are-resolved) and [here](https://github.com/HenrikBengtsson/future.batchtools#choosing-batchtools-backend).

## `"Makefile`"

Here, `drake` actually writes, configures, and runs a proper [`Makefile`](https://www.gnu.org/software/make/) to run the targets.

```{r futurebatchtools3, eval = FALSE}
make(my_plan, parallelism = "Makefile", jobs = 2)
```

You can configure both the Unix command that runs the [`Makefile`](https://www.gnu.org/software/make/) and the command line arguments passed to it.

```{r touchsilent, eval = FALSE}
make(
  my_plan,
  parallelism = "Makefile",
  command = "lsmake",
  args = c("--touch", "--silent")
)
```

There are more customization options in `make()`, such as the `recipe_command` argument.

```{r recipe2, eval = FALSE}
make(my_plan, parallelism = "Makefile", jobs = 4,
  recipe_command = "R -e 'R_RECIPE' -q")
```

See the help files of individual functions for details.

```{r defaultmakecommandfunction}
default_Makefile_command()

default_recipe_command()

r_recipe_wildcard()

Makefile_recipe(
  recipe_command = "R -e 'R_RECIPE' -q",
  target = "this_target",
  cache_path = "custom_cache"
)
```

To deploy workers to a cluster, you need to supply the [`Makefile`](https://www.gnu.org/software/make/) with a custom shell script that launches cluster jobs. Use the `shell_file()` function to write an example compatible with the [Univa Grid Engine](http://www.univa.com/products/). You will probably need to configure it manually. Suppose our file is `shell.sh`.

<pre><code>#!/bin/bash
shift
echo "module load R; $*" | qsub -sync y -cwd -j y
</code></pre>

You will need to set permissions to allow execution. In the Linux command line, this is straightforward.

<pre><code>$ chmod +x shell.sh 
</code></pre>

When you actually call `make()`, use the `prepend` argument to write a line at the top of the [`Makefile`](https://www.gnu.org/software/make/) to reference your shell file.

```{r hpcprepend, eval = FALSE}
make(my_plan, parallelism = "Makefile", jobs = 2, prepend = "SHELL=./shell.sh")
```

[SLURM](https://slurm.schedmd.com/) users may be able to [invoke `srun` and dispense with `shell.sh` altogether](http://plindenbaum.blogspot.com/2014/09/parallelizing-gnu-make-4-in-slurm.html), though success may vary depending on the SLURM system. You will probably also need to set resource allocation parameters governing memory, runtime, etc. See `man srun` for the possible `.SHELLFLAGS`.

```{r cluster, eval = FALSE}
make(
  my_plan,
  parallelism = "Makefile",
  jobs = 2,
  prepend = c(
    "SHELL=srun",
    ".SHELLFLAGS=-N1 -n1 bash -c"
  )
)
```

# Scheduling algorithms

## Persistent scheduling

Backends  "mclapply", "parLapply", and "future_lapply" launch persistent workers.

```{r persist, eval = FALSE}
make(my_plan, parallelism = "mclapply", jobs = 2)
make(my_plan, parallelism = "parLapply", jobs = 2)
future::plan(future::multisession)
make(my_plan, parallelism = "future_lapply", jobs = 2)
```

In each of these calls to `make()`, three processes launch: two workers and one master. Whenever a worker is idle, the master assigns it the next available target (whose dependencies have been built). The workers keep running until there are no more targets to build. The following video demonstrates the concept.

<iframe width="560" height="315" src="https://www.youtube.com/embed/oCvrEFQOYoA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

The `predict_runtime()` and `predict_load_balancing()` functions emulate persistent workers, and the predictions also apply to transient workers. See the [timing guide](https://ropensci.github.io/drake/articles/timing.html) for a demonstration.

## Transient scheduling

Persistent workers are great because they minimize overhead: all the workers are created at the beginning, and then you never have to create any more for the rest of the runthrough. Unfortunately, computing clusters usually limit the amount of time each worker can stay running. That is why `drake` also supports transient workers in backends `"future"` and `"Makefile"`. Here, the master process creates a new worker for each target individually, and the worker dies after it finishes its single target. For the `"future"` backend, the master is just the existing process calling `make()`. The following video demonstrates the concept.

<iframe width="560" height="315" src="https://www.youtube.com/embed/aiAdaqKX_a8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

```{r transient, eval = FALSE}
future::plan(future::multisession)
make(my_plan, parallelism = "future", jobs = 2)
make(my_plan, parallelism = "Makefile", jobs = 2)
```

## Staged scheduling

Backends `"mclapply_staged"` and `"parLapply_staged"` support staged scheduling.

```{r staged, eval = FALSE}
make(my_plan, parallelism = "mclapply_staged", jobs = 2)
make(my_plan, parallelism = "parLapply_staged", jobs = 2)
```

Here, the dependency network is divided into separate stages of conditionally independent targets. Within each stage, `drake` uses `mclapply()` or `parLapply()` to process the targets in parallel. Stages run one after the other, so the slowest target in the current stage needs to complete before the next stage begins. So we lose a lot of parallel efficiency. The following video demonstrates the major drawback.

<iframe width="560" height="315" src="https://www.youtube.com/embed/EEK-5sIixf0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

However, because there is no formal master process in each stage, overhead is extremely low. This lack of overhead can make staged parallelism a great choice for projects with a small number of large stages: tall dependency graphs with most of the work in the tallest stages.

```{r staged2}
library(dplyr)
library(drake)

N <- 500

gen_data <- function() {
  tibble(a = seq_len(N), b = 1, c = 2, d = 3)
}

plan_data <- drake_plan(
  data = gen_data()
)

plan_sub <-
  gen_data() %>%
  transmute(
    target = paste0("data", a),
    command = paste0("data[", a, ", ]")
  )

plan <- bind_rows(plan_data, plan_sub)
plan
```

```{r staged3, eval = FALSE}
config <- drake_config(plan)
vis_drake_graph(plan)
```

<iframe
src = "https://ropensci.github.io/drake/images/staged.html"
width = "100%" height = "600px" allowtransparency="true"
style="border: none; box-shadow: none">
</iframe>

# Final thoughts

## Debugging

For large workflows, downsizing and debugging tools become super important. See the ["debug" vignette](https://github.com/ropensci/drake/blob/master/vignettes/debug.Rmd) for help on diagnosing problems with a workflow. [Triggers](https://github.com/ropensci/drake/blob/master/vignettes/debug.Rmd#test-with-triggers) and [cached error logs](https://github.com/ropensci/drake/blob/master/vignettes/debug.Rmd#diagnose-failures) especially speed the development and testing process.

## Drake as an ordinary job scheduler

If you do not care about reproducibility and you want `drake` to be an ordinary job scheduler, consider using [alternative triggers](https://github.com/ropensci/drake/blob/master/vignettes/debug.Rmd#test-with-triggers).

```{r triggerparallel, eval = FALSE}
load_mtcars_example()
make(my_plan, trigger = "missing") # Also consider "always".
```

Above, `drake` only builds the missing targets. This skips much of the [time-consuming hashing](https://github.com/ropensci/drake/blob/master/vignettes/storage.Rmd#hash-algorithms) that ordinarily detects which targets are out of date.

## More resources

See the [timing vignette](https://github.com/ropensci/drake/blob/master/vignettes/timing.Rmd) for explanations of functions `predict_runtime()` and `predict_load_balancing()`, which can help you plan and strategize deployment.


```{r endofline_quickstart, echo = F}
clean(destroy = TRUE, verbose = FALSE)
unlink(c("Makefile", "report.Rmd", "shell.sh", "STDIN.o*", "Thumbs.db"))
```
