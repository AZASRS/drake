---
title: "Time: logging, prediction, and strategy"
author: "Will Landau"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    toc: true
    number_sections: true
vignette: >
  %\VignetteIndexEntry{timing}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r suppression_timing, echo = F}
suppressMessages(suppressWarnings(library(drake)))
clean(destroy = TRUE, verbose = FALSE)
unlink(c("Makefile", "report.Rmd", "shell.sh", "STDIN.o*", "Thumbs.db"))
knitr::opts_chunk$set(
  collapse = TRUE,
  error = TRUE,
  warning = TRUE
)
```

Thanks to [Jasper Clarkberg](https://github.com/dapperjapper), `drake` records how long it takes to build each target. For large projects that take hours or days to run, this feature becomes important for planning and execution.

```{r timing_intro}
library(drake)
load_mtcars_example() # Get the code with drake_example("mtcars").
make(my_plan, jobs = 2)

build_times(digits = 8) # From the cache.

# `dplyr`-style `tidyselect` commands
build_times(starts_with("coef"), digits = 8)

build_times(digits = 8, targets_only = TRUE)
```

For `drake` version 4.1.0 and earlier, `build_times()` just measures the elapsed runtime of each command in `my_plan$command`. For later versions, the build times also account for all the internal operations in `drake:::build()`, such as [storage and hashing](https://github.com/ropensci/drake/blob/master/vignettes/storage.Rmd).

# Predict total runtime

Drake uses these times to predict the runtime of the next `make()`. At this moment, everything is up to date in the current example, so the next `make()` should be fast. Here, we only factor in the times of the formal targets in the workflow plan, excluding any imports.

```{r predict_runtime}
config <- drake_config(my_plan, verbose = FALSE)
predict_runtime(config, targets_only = TRUE)
```

Suppose we change a dependency to make some targets out of date. Now, even though, the next `make()` should take a little longer.

```{r changedep_timing}
reg2 <- function(d){
  d$x3 <- d$x ^ 3
  lm(y ~ x3, data = d)
}

predict_runtime(config, targets_only = TRUE)
```

But what if you plan on starting from scratch next time, either after `clean()` or with `make(..., trigger = "always")`?

```{r predict_runtime_scratch}
predict_runtime(config, from_scratch = TRUE, targets_only = TRUE)
```

# Strategize your high-performance computing

Let's say you are scaling up your workflow. You just put bigger data and heavier computation in your custom code, and the next time you run `make()`, your targets will take much longer to build. In fact, you estimate that every target except for your R Markdown report will take two hours to complete. Let's write down these known times in seconds.

```{r knowntimes}
known_times <- c(5, rep(7200, nrow(my_plan) - 1))
names(known_times) <- c(file_store("report.md"), my_plan$target[-1])
known_times
```

How many parallel jobs should you use in the next `make()`? The `predict_runtime()` function can help you decide. `predict_runtime(jobs = n)` simulates how the targets are be distributed among `n` workers and then reports the virtual runtime of the busiest worker.

```{r predictjobs}
time <- c()
for (jobs in 1:12){
  time[jobs] <- predict_runtime(
    config,
    jobs = jobs,
    from_scratch = TRUE,
    known_times = known_times
  )
}
library(ggplot2)
ggplot(data.frame(time = time / 3600, jobs = ordered(1:12), group = 1)) +
  geom_line(aes(x = jobs, y = time, group = group)) +
  scale_y_continuous(breaks = 0:10 * 4) +
  theme_gray(12) +
  xlab("`jobs` argument to make()") +
  ylab("Predicted runtime of make() (hours)")
```

Now, let's say your laptop has 4 single-threaded cores, so you cannot realistically see major performance improvements beyond 4 jobs. Theoretically, if your assumptions about `known_times` are correct, you should still be able to finish in about 8 hours.

```{r laptop, eval = FALSE}
make(my_plan, jobs = 4)
```

But you can chop the time in half if you set `jobs` to 7 and send your targets to a computing cluster. See the [parallelism computing guide](https://ropensci.github.io/drake/articles/parallelism.html) for more on how this works.

```{r timing_future, eval = FALSE}
# Log into the cluster's head node / login node and start R.
$ ssh you@login_node.your_cluster.com
# Deployment to the cluster happens through future and future.batchtools.
library(future.batchtools)
# Write batchtools.slurm.tmpl that tells future.batchtools
# how to talk to the compute nodes on the cluster.
# You should modify and tailor it to your specific computing resource needs.
# For example, make sure your wall time limit is over 2 hours.
drake_batchtools_tmpl_file("slurm")
# Connect future to the cluster.
future::plan(batchtools_slurm, template = "batchtools.slurm.tmpl")
# Each target goes to its own node on the cluster,
# with up to 7 targets running at a time.
make(my_plan, parallelism = "future", jobs = 7, verbose = 4)
```

But why 7 jobs? Why not 8? 8 seems more intuitive when you look at the dependency graph and see a column of 8 targets.

<iframe
src = "https://ropensci.github.io/drake/images/outdated.html"
width = "100%" height = "600px" allowtransparency="true"
style="border: none; box-shadow: none">
</iframe>

The key is to think in terms of load balancing for persistent workers (transient workers should be similar here). Each free worker moves to each next free target as soon as possible, minimizing idle time. This splits the 14 computationally expensive targets evenly over the 7 workers. If you add another worker, you will still have workers with two expensive targets, each of which will run for 4 hours.

```{r predict_load}
balance <- predict_load_balancing(
  config,
  jobs = 7,
  from_scratch = TRUE,
  targets_only = TRUE,
  known_times = known_times
)
balance
max(balance$time) / 3600
balance$targets
```

But be warned: if you use the staged parallel backends `make(parallelism = "mclapply_staged")` or `make(parallelism = "parLapply_staged")`, the maximum number of useful jobs really will be 8. Here, if all targets are out of date, the dependency graph is divided into conditionally independent columns and parallelism is only applied in within each column. If some targets are already up to date, those targets are skipped, and each stage is packed with as many outdated targets as possible. Either way, each stage needs to wait for the last target to finish. So if you have computationally expensive targets, this approach is not recommended. However, staged parallelism remains a fast, low-overhead option for workflows with a large number of small, conditionally independent targets.

```{r endofline_timing, echo = F}
clean(destroy = TRUE, verbose = FALSE)
unlink(c("Makefile", "report.Rmd", "shell.sh", "STDIN.o*", "Thumbs.db"))
```
